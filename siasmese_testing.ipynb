{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Software\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomVerticalFlip(p=1),\n",
    "    transforms.Resize((64, 64)),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_base(input_string):\n",
    "    letters = re.findall(r'\\D', input_string)\n",
    "    return ''.join(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFolderFaces(Dataset):\n",
    "    def __init__(self,root:str,transform=None,unique:bool = False,cv2_color=cv2.COLOR_BGR2GRAY, take_length=False,length=100) -> None:\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.cv2_color=cv2_color\n",
    "        self.unique = unique\n",
    "        self.take_length = take_length\n",
    "        self.length=length    \n",
    "        self.paths = sorted(list(map(str,Path(self.root).glob(\"*.jpg\"))))\n",
    "        self.unique_paths = list(set(find_base(i.split(\".\")[0])+'0.jpg' for i in self.paths))\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        if(self.take_length):\n",
    "            return self.length\n",
    "        else:\n",
    "            return len(self.paths) if not self.unique else len(self.unique_paths)\n",
    "    \n",
    "    def __getitem__(self, index:int) -> Tuple[torch.Tensor,str]:\n",
    "        \n",
    "        if not self.unique:\n",
    "            pths = self.paths\n",
    "        else:\n",
    "            pths = self.unique_paths\n",
    "            \n",
    "        img1_path = pths[index]\n",
    "        name= str(Path(img1_path).stem)\n",
    "        img1 = cv2.imread(img1_path)\n",
    "        if self.cv2_color:\n",
    "            img1 = cv2.cvtColor(img1,self.cv2_color)\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "        return img1,name\n",
    "\n",
    "\n",
    "test_dataset_p = CustomFolderFaces('./dataset',train_transform,unique=True)\n",
    "test_dataloader_p = DataLoader(test_dataset_p, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        # euclidian distance\n",
    "        diff = x0 - x1\n",
    "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "\n",
    "        mdist = self.margin - dist\n",
    "        dist = torch.clamp(mdist, min=0.0)\n",
    "        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=5,stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "        )\n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.LazyLinear(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LazyLinear(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1)\n",
    "        x = self.fc(x)\n",
    "        # x = nn.functional.normalize(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pt\",map_location=torch.device('cpu')))\n",
    "# model = Network()\n",
    "# model.load_state_dict(torch.load(\"model_triplet.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_contrastive(input:torch.Tensor)->Tuple:\n",
    "    x0 = test_transform(input)\n",
    "    x0 = x0.unsqueeze(0)\n",
    "    distances=[]\n",
    "    img_names=[]\n",
    "    for i, data in enumerate(test_dataloader_p):\n",
    "        x1,name = data\n",
    "        # Example batch is a list containing 3x1 images&labels [0] - [8,1,64,64], [1] - [8,1,64,64],[2] - [8,]\n",
    "        # If the label is 0, it means that it is not the same person, label is 1, same person in both images\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            # x0,x1 = x0.cuda(),x1.cuda()\n",
    "            out1,out2 = model(x0,x1)\n",
    "            res = F.pairwise_distance(out1, out2)\n",
    "            # x0,x1 = x0.cpu(),x1.cpu()\n",
    "            distances.append(res)\n",
    "            img_names.append(name)\n",
    "    \n",
    "    idx = torch.argmin(torch.tensor(distances))\n",
    "    # print(idx,distances,img_names)\n",
    "    return distances[idx],img_names[idx]\n",
    "\n",
    "def prediction_triplet(input:torch.Tensor)->Tuple:\n",
    "    x0 = test_transform(input)\n",
    "    x0 = x0.unsqueeze(0)\n",
    "    distances=[]\n",
    "    img_names=[]\n",
    "    for i, data in enumerate(test_dataloader_p):\n",
    "        x1,name = data\n",
    "        # Example batch is a list containing 3x1 images&labels [0] - [8,1,64,64], [1] - [8,1,64,64],[2] - [8,]\n",
    "        # If the label is 0, it means that it is not the same person, label is 1, same person in both images\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            # x0,x1 = x0.cuda(),x1.cuda()\n",
    "            out1= model(x0)\n",
    "            out2= model(x1)\n",
    "            res = F.pairwise_distance(out1, out2)\n",
    "            # x0,x1 = x0.cpu(),x1.cpu()\n",
    "            distances.append(res)\n",
    "            img_names.append(name)\n",
    "    \n",
    "    idx = torch.argmin(torch.tensor(distances))\n",
    "    # print(idx,distances,img_names)\n",
    "    return distances[idx],img_names[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Software\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Software\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     11\u001b[0m _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# # resize the frame for portrait video\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# frame = cv2.resize(frame, (64,64))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# convert to RGB\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m frame_gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,frame)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# detector is defined above, otherwise uncomment\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('ankith1.jpg')\n",
    "\n",
    "# width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\n",
    "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# print(width, height,fps) \n",
    "colors = [(0, 255, 0), (0, 0, 255), (255, 0, 0)]\n",
    "\n",
    "while cap.isOpened():\n",
    "    # read frame\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    # # resize the frame for portrait video\n",
    "    # frame = cv2.resize(frame, (64,64))\n",
    "\n",
    "    # convert to RGB\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('test',frame)\n",
    "\n",
    "\n",
    "    # detector is defined above, otherwise uncomment\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    faces = detector.detect_faces(frame)\n",
    "    if len(faces)!=0:\n",
    "        for i,face in enumerate(faces): # for each face found in single frame\n",
    "            x, y, w, h = face['box']\n",
    "            img = cv2.rectangle(frame, (x, y), (x+w, y+h), colors[i%len(colors)], 2)\n",
    "            \n",
    "            dist,name = prediction_contrastive(frame_gray[y:y+h,x:x+w])\n",
    "            # dist,name = prediction_triplet(frame_gray[y:y+h,x:x+w])\n",
    "\n",
    "            cv2.putText(frame, f'{name} - {dist}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        cv2.imshow('test',frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
